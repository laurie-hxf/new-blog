---
title: 'Lecture5 Neural Networks'
publishDate: 2025-02-19
description: 'UMich EECS 498-007 Deep learning-Neural Networks'
tags:
 - DeepLearning
 - NeuralNetworks
language: 'Chinese'
heroImage: { src: './截屏2025-02-19 20.14.14.png', color: '#D58388' }
---

## Problem: Linear Classifiers aren’t that powerful

### #1 Feature Transforms
我们之前讲过linear classifier是线性的，所以他不能识别一些非线性的图案,但是我们可以通过一些方法将我们要识别的数据转化成线性的，这样我们就可以利用Linear Classifiers来识别
![alt text](./截屏2025-02-09%2015.51.34.png)

### #2 Color Histogram
我们将图片转化为颜色直方图，这样可以忽略物体在照片中的空间位置，根据颜色来识别物体，把图片中的颜色转化为向量然后训练。
![alt text](./截屏2025-02-09%2015.57.34.png)

### #3 Histogram of Oriented Gradients (HoG)

HoG的核心思想是**通过捕捉图像中物体轮廓和边缘的形状信息**来提取特征。图像的梯度方向和幅度可以反映出物体的边缘、纹理等结构信息，这些信息对物体的识别和分类非常重要。

![alt text](./截屏2025-02-09%2015.59.11.png)

### #4 Bag of Words (Data-Driven!)

我们冲数据集中每个图片提取一些块，然后这些块组成一个 codebook，然后我们就可以将图片表示为这个图片有多少个codebook中这一个块的个数，以此类推
![alt text](./截屏2025-02-09%2016.13.30.png)

---
## Neural Networks

上面说的这些都是图片的某些特征，我们不只用单独的特征来识别图片，我们用多个特征组合起来形成一个长特征向量来表示一张图片
![alt text](./截屏2025-02-09%2016.17.44.png)
以前的想法就是将一个系统分为两部分，一部分就是特征的提取，一部分就是训练部分
神经网络的动机就是最大化提高图像分类的能力，最大的区别就是他用一整个系统共同来调整这两部分

现在就看一下神经网络的简单例子

$$
Linear\ Classifiers:f = Wx+b
$$
$$2
\ layer\ Neural\ Networks:f = W_2max(0,W_1x+b_1)+b_2
$$
### Fully-connected neural network

由于x中的每个元素都会对h中的每个元素造成影响，h中的也会对s造成影响，神经网络的每一层都是相互连接的，所以将这种神经网络称为Fully-connected neural network，也叫多层感知机(MLP)
![alt text](./截屏2025-02-09%2016.41.24.png)

max那部分被称为**激活函数**，如果我们没有那部分，我们的函数变为$s=W_2W_1x$ 这时他仍然是一个Linear Classifiers，所以我们要在两个矩阵之间加一个非线性的函数。当然这种激活函数可以有很多种，不只是max这种，但max是用的最广泛的激活函数。

![alt text](./截屏2025-02-19%2020.52.15.png)

激活函数最重要的作用就是将分类可以变的不再线性，比如原本没有激活函数的时候
![alt text](./截屏2025-02-09%2022.39.21.png)
然后通过激活函数之后，B,C,D中的数据全部被投影到坐标轴中
![alt text](./截屏2025-02-19%2019.48.00.png)
于是再通过线性分类(也就是第二个W)就可以将这些区分开来，从而这种做法就可以达到非线性的区分。这里的神经网络就是2层
![alt text](./截屏2025-02-19%2019.49.29.png)

### 神经网络的由来

就因为这种结构启发与神经元，所以叫神经网络。又图中每一个hidden layer其实相当于每一层的权重矩阵以及激活函数，偏移量。中间的一个个圆圈在神经网络中也被称为神经元（Neuron），“节点”（Node）或“单元”（Unit），他本质上就是权重矩阵中的一行，偏移量中的一个数。

![alt text](./截屏2025-02-19%2020.14.14.png)

### Regularize

前面我们说过正则化的目的之一是防止过拟合，在神经网络中我们可以看到层数越多他模拟的就越精细，他也就越容易过拟合。但是这时我们不选择减少层数来实现正则化，而是添加偏移量。从而使我们的决策边界变得平滑。

![alt text](./截屏2025-02-19%2020.13.08.png)

### Universal Approximation(万能逼近定理)

这里想说的就是，适当的人工神经网络能够逼近任何连续函数，只要该网络具有足够的宽度（即足够多的神经元）和合适的激活函数。

比如我们的ReLU激活函数，每四个神经元他就可以帮助形成 其中一种bump function。bump function具有强烈的局部性和光滑性，通常用来进行局部分析或构造光滑函数的近似。
![alt text](./截屏2025-02-19%2020.53.18.png)

当我们把多个bump function连接在一起的时候他就可以模拟非线性的函数，这就是神经网络在理论上具备了近似任何函数的能力的原因

![alt text](./截屏2025-02-19%2020.56.33.png)

但是这只是理论上，实际上Universal approximation 没有告诉我们，我们是否可以通过SGD来学习到任何函数以及我们需要多大的数据来训练一个函数。

所以这些局限性促使了很多新的研究方向，包括更高效的训练算法、优化技术、正则化方法、以及小样本学习等